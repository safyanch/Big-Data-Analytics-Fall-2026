{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178b1ea-4d9e-4a42-878b-708f859da117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c277238-3900-45ac-abfe-2b81e4cf0f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr.Sahib\\anaconda3\\envs\\spark1\\python.exe\n",
      "C:\\Users\\Dr.Sahib\\anaconda3\\envs\\spark1\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"PYSPARK_PYTHON\"))\n",
    "print(os.environ.get(\"PYSPARK_DRIVER_PYTHON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd865c6d-a3b4-4471-ae9d-b9d4c63c2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = r\"C:\\Users\\Dr.Sahib\\anaconda3\\envs\\spark1\\python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\Dr.Sahib\\anaconda3\\envs\\spark1\\python.exe\"\n",
    "os.environ['PYSPARK_PYTHONHASHSEED'] = '0'  # makes execution deterministic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c57b0bd1-fb82-44e0-9349-78893a911765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr.Sahib\\anaconda3\\envs\\spark1\\python.exe\n",
      "C:\\Users\\Dr.Sahib\\anaconda3\\envs\\spark1\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"PYSPARK_PYTHON\"))\n",
    "print(os.environ.get(\"PYSPARK_DRIVER_PYTHON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb742d01-8a77-403d-9fac-eaa09a9f9648",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\session.py:562\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    560\u001b[39m     session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m     module = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_j_spark_session_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m     module.applyModifiableSettings(session._jsparkSession, \u001b[38;5;28mself\u001b[39m._options)\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\session.py:673\u001b[39m, in \u001b[36mSparkSession._get_j_spark_session_module\u001b[39m\u001b[34m(jvm)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_j_spark_session_module\u001b[39m(jvm: \u001b[33m\"\u001b[39m\u001b[33mJVMView\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mJavaObject\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.sql.classic.SparkSession$\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\java_gateway.py:1752\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == UserHelpAutoCompletion.KEY:\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1755\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n\u001b[32m   1757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m._gateway_client, jvm_id=\u001b[38;5;28mself\u001b[39m._id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28mself\u001b[39m.socket.connect((\u001b[38;5;28mself\u001b[39m.java_address, \u001b[38;5;28mself\u001b[39m.java_port))\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eec9e3-54b1-4126-ba2d-8154799fec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b94134-b69b-49c5-afd5-4a54c31459a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f9041-c0d9-46e9-b685-e61a2e118237",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([1,2,3]).map(lambda x: x+1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbc486-6271-4347-ab15-1499462bc1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrDD=sc.parallelize(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd63950-c01b-4619-9ac9-a042d795b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43518c6f-6eaa-4c54-a3f0-9d21d7a5013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828d91e-749b-4682-8b13-e1742d455298",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD=sc.textFile(r\"file:///j:\\sample1\\Data8277.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06d65f-e930-4e4f-b392-43c11436f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef66264-f6f2-4f5d-af97-c230a5be2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD1=dataRDD.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b34b4c-b900-4c9a-873c-c42e1c54fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f6520-1026-43f6-8981-c57256445495",
   "metadata": {},
   "source": [
    "# Lazy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f810feb-13c2-4954-ab75-fe4e7d95effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d03680-0481-4c30-ae99-3c8f1c7104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda x: x*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfbd640-9f63-4799-9ad1-2155b7e247ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.filter(lambda x: x > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a57cff-3047-47f2-a559-5d12796cd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117333c1-ef8a-4e55-bb1e-ba9064ae63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d90f7-1802-46e5-aa77-87f082efb659",
   "metadata": {},
   "source": [
    "# Narrow and Wide Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a82005-555c-44ad-a6c1-40faa7f37994",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    [15, 35, 23],\n",
    "    [14, 35, 78],\n",
    "    [66, 21, 45],\n",
    "    [22, 41, 78],\n",
    "    [32, 22, 11],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b40c92d-5629-4b35-8483-5211bb3087cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(pages, len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c19df6-746b-4cc8-b52b-27af5f9133ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c90602-093f-41bc-b345-50e26c1521be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_each_file = rdd.map(lambda page: sum(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef7666-5445-4358-af75-f7c5b405f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_each_file.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1271cc0-132d-4f09-a962-e8b3253846c4",
   "metadata": {},
   "source": [
    "# wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f37ab-4e35-40e3-8715-56ae245bdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    [15, 35, 23],\n",
    "    [14, 35, 78],\n",
    "    [66, 21, 45],\n",
    "    [22, 41, 78],\n",
    "    [32, 22, 11],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db24ca7-1128-4a60-9a85-04e2a86a8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(pages, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058ebef-8b64-4cac-b03b-4a0994c0a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd704e5-d85c-485d-9cc9-1360e51899ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each page into (key, value) pairs\n",
    "# A = first value, B = second, C = third\n",
    "kv_rdd = rdd.flatMap(lambda page: [\n",
    "    (\"A\", page[0]),\n",
    "    (\"B\", page[1]),\n",
    "    (\"C\", page[2])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3dc20-3d8e-4642-b03a-edbfe3176077",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kv_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39d715-b931-4d74-804e-146d526fc6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide transformation (shuffle happens here)\n",
    "totals_ABC = kv_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(totals_ABC.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce7d13-79df-4f1e-81e1-e38c479be853",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sort=totals_ABC.sortBy(lambda x: x)\n",
    "print(total_sort.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1b736-004e-4859-afcb-c900f801fe19",
   "metadata": {},
   "source": [
    "# Spark Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3ec94-0333-4966-a749-005bcdebb7bf",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c212b33-0db5-43c7-bdd3-b1bc4f7e3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datardd=sc.textFile(r\"file:///J:/sample.txt\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd08476-927f-489d-83d7-451cf1f91c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "datardd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1734720-44ca-42dc-8dd8-b28427353376",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmrdd=datardd.flatMap(lambda x:x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e47fa-6c60-4a3a-98fe-3c1c5fa33c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrdd= fmrdd.map(lambda x:(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4906e-c84f-4081-a5c2-b32158a76e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mrdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e2a727-e4ed-40c4-a0a5-852c7385f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=mrdd.reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e2624-f5b5-4599-9e83-00d7f3a92e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(res.collect())\n",
    "res.saveAsTextFile(r\"J:/wordCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0531b67-f582-493c-9870-a718099feed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef31c2b-4d07-4aa6-8882-8a95c9c7fa80",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0947174-2fb1-496a-a1b0-3d6f525da05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = r\"C:\\Users\\Dr.Sahib\\anaconda3\\envs\\spark1\\python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\Dr.Sahib\\anaconda3\\envs\\spark1\\python.exe\"\n",
    "os.environ['PYSPARK_PYTHONHASHSEED'] = '0'  # makes execution deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ebe8e7-a594-4f84-a31f-efa7f72fc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ed2fab-26d3-4c22-b0d7-32c91b457338",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085695e3-a5e7-44d4-a8f0-f7049f5e64cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'Bob', 'IN', 'India'), (1, 'Alice', 'US', 'United States'), (4, 'David', 'US', 'United States'), (3, 'Charlie', 'FR', 'France')]\n"
     ]
    }
   ],
   "source": [
    "country_data = [\n",
    "    (\"US\", \"United States\"),\n",
    "    (\"IN\", \"India\"),\n",
    "    (\"FR\", \"France\")\n",
    "]\n",
    "\n",
    "users = [\n",
    "    (1, \"Alice\", \"US\"),\n",
    "    (2, \"Bob\", \"IN\"),\n",
    "    (3, \"Charlie\", \"FR\"),\n",
    "    (4, \"David\", \"US\")\n",
    "]\n",
    "\n",
    "users_rdd = sc.parallelize(users)\n",
    "countries_rdd = sc.parallelize(country_data)\n",
    "\n",
    "users_kv = users_rdd.map(lambda x: (x[2], (x[0], x[1])))\n",
    "joined_rdd = users_kv.join(countries_rdd)\n",
    "\n",
    "final_rdd = joined_rdd.map(\n",
    "    lambda x: (x[1][0][0], x[1][0][1], x[0], x[1][1])\n",
    ")\n",
    "\n",
    "print(final_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ac5e9-c6bd-4872-85e9-7f5834ed1dd2",
   "metadata": {},
   "source": [
    "# With Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04553684-d803-4b32-bd07-4642567125e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Alice', 'US', 'United States')\n",
      "(2, 'Bob', 'IN', 'India')\n",
      "(3, 'Charlie', 'FR', 'France')\n",
      "(4, 'David', 'US', 'United States')\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create SparkContext\n",
    "#sc = SparkContext(\"local\", \"NoBroadcastJoinRDD\")\n",
    "\n",
    "# Small dataset: country codes (regular dictionary, not broadcasted)\n",
    "country_data = {\n",
    "    \"US\": \"United States\",\n",
    "    \"IN\": \"India\",\n",
    "    \"FR\": \"France\"\n",
    "}\n",
    "\n",
    "# Large dataset: users with country codes\n",
    "users = [\n",
    "    (1, \"Alice\", \"US\"),\n",
    "    (2, \"Bob\", \"IN\"),\n",
    "    (3, \"Charlie\", \"FR\"),\n",
    "    (4, \"David\", \"US\")\n",
    "]\n",
    "\n",
    "# Parallelize the large dataset\n",
    "users_rdd = sc.parallelize(users)\n",
    "\n",
    "# Perform join using the regular dictionary (not broadcasted)\n",
    "def map_with_country_name(record):\n",
    "    user_id, name, country_code = record\n",
    "    country_name = country_data.get(country_code, \"Unknown\")  # Accessing regular dictionary\n",
    "    return (user_id, name, country_code, country_name)\n",
    "\n",
    "joined_rdd = users_rdd.map(map_with_country_name)\n",
    "\n",
    "# Collect and print result\n",
    "for row in joined_rdd.collect():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee8ef0-7aff-416f-b4dd-2a8ef6d214d2",
   "metadata": {},
   "source": [
    "# with BroadCast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ca56c9-48f6-469c-b0f2-82fc09bca04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Alice', 'US', 'United States')\n",
      "(2, 'Bob', 'IN', 'India')\n",
      "(3, 'Charlie', 'FR', 'France')\n",
      "(4, 'David', 'US', 'United States')\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create SparkContext\n",
    "#sc = SparkContext(\"local\", \"BroadcastJoinRDD\")\n",
    "\n",
    "# Small dataset: country codes (to broadcast)\n",
    "country_data = {\n",
    "    \"US\": \"United States\",\n",
    "    \"IN\": \"India\",\n",
    "    \"FR\": \"France\"\n",
    "}\n",
    "\n",
    "# Broadcast the small dictionary\n",
    "broadcast_countries = sc.broadcast(country_data)\n",
    "\n",
    "# Large dataset: users with country codes\n",
    "users = [\n",
    "    (1, \"Alice\", \"US\"),\n",
    "    (2, \"Bob\", \"IN\"),\n",
    "    (3, \"Charlie\", \"FR\"),\n",
    "    (4, \"David\", \"US\")\n",
    "]\n",
    "\n",
    "# Parallelize the large dataset\n",
    "users_rdd = sc.parallelize(users)\n",
    "\n",
    "# Perform join using the broadcast variable\n",
    "def map_with_country_name(record):\n",
    "    user_id, name, country_code = record\n",
    "    country_name = broadcast_countries.value.get(country_code, \"Unknown\")\n",
    "    return (user_id, name, country_code, country_name)\n",
    "\n",
    "joined_rdd = users_rdd.map(map_with_country_name)\n",
    "\n",
    "# Collect and print result\n",
    "for row in joined_rdd.collect():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb5367-6406-4354-82ee-0c9476f26688",
   "metadata": {},
   "source": [
    "# Accumlator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6baff0-c52f-49ce-90ed-eb6479a66ceb",
   "metadata": {},
   "source": [
    "# with Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e490da0-1ce3-431d-aa4d-f05fcb8ffd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create SparkContext\n",
    "sc = SparkContext(\"local\", \"CountActionsExample\")\n",
    "\n",
    "# Sample log data\n",
    "log_data = [\n",
    "    \"INFO Server started\",\n",
    "    \"WARN Disk space low\",\n",
    "    \"ERROR Connection failed\",\n",
    "    \"INFO Request processed\",\n",
    "    \"ERROR Disk read failed\",\n",
    "    \"WARN CPU usage high\"\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "logs_rdd = sc.parallelize(log_data)\n",
    "\n",
    "# Filter and count ERROR lines\n",
    "error_count = logs_rdd.filter(lambda line: \"ERROR\" in line).count()\n",
    "\n",
    "# Filter and count WARN lines\n",
    "warn_count = logs_rdd.filter(lambda line: \"WARN\" in line).count()\n",
    "\n",
    "# Print results\n",
    "print(\"Number of ERROR lines:\", error_count)\n",
    "print(\"Number of WARN lines:\", warn_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2b2f6-6464-43f7-bb9a-f45088220904",
   "metadata": {},
   "source": [
    "# with Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1be968-dc98-4d15-81d3-a8ee1dc346f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3331339001.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom pyspark import SparkContext\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "    from pyspark import SparkContext\n",
    "\n",
    "# Create SparkContext\n",
    "sc = SparkContext(\"local\", \"GroupByKeyLogLevelCount\")\n",
    "\n",
    "# Sample log data\n",
    "log_data = [\n",
    "    \"INFO Server started\",\n",
    "    \"WARN Disk space low\",\n",
    "    \"ERROR Connection failed\",\n",
    "    \"INFO Request processed\",\n",
    "    \"ERROR Disk read failed\",\n",
    "    \"WARN CPU usage high\"\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "logs_rdd = sc.parallelize(log_data)\n",
    "\n",
    "# Filter relevant levels and map to (level, 1)\n",
    "level_pairs = logs_rdd.flatMap(lambda line:\n",
    "    [(\"ERROR\", 1)] if \"ERROR\" in line else\n",
    "    [(\"WARN\", 1)] if \"WARN\" in line else [])\n",
    "\n",
    "# Use groupByKey to group and count\n",
    "grouped = level_pairs.groupByKey()\n",
    "result = grouped.mapValues(lambda counts: sum(counts))\n",
    "\n",
    "# Collect and print the result\n",
    "for level, count in result.collect():\n",
    "    print(f\"{level}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc77c6-8644-4834-ba60-a66a16496c9a",
   "metadata": {},
   "source": [
    "# with Accumlator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed380645-7249-42ba-a65b-2208154d0f1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[2]) created by getOrCreate at C:\\Users\\Dr.Sahib\\AppData\\Local\\Temp\\ipykernel_19312\\2236833996.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Create SparkContext\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAccumulatorMultipleExample\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Sample log data\u001b[39;00m\n\u001b[32m      7\u001b[39m log_data = [\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mINFO Server started\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWARN Disk space low\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWARN CPU usage high\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\core\\context.py:206\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    209\u001b[39m         master,\n\u001b[32m    210\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         memory_profiler_cls,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\core\\context.py:476\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    473\u001b[39m     callsite = SparkContext._active_spark_context._callsite\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    477\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot run multiple SparkContexts at once; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    478\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    479\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    480\u001b[39m         % (\n\u001b[32m    481\u001b[39m             currentAppName,\n\u001b[32m    482\u001b[39m             currentMaster,\n\u001b[32m    483\u001b[39m             callsite.function,\n\u001b[32m    484\u001b[39m             callsite.file,\n\u001b[32m    485\u001b[39m             callsite.linenum,\n\u001b[32m    486\u001b[39m         )\n\u001b[32m    487\u001b[39m     )\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m     SparkContext._active_spark_context = instance\n",
      "\u001b[31mValueError\u001b[39m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[2]) created by getOrCreate at C:\\Users\\Dr.Sahib\\AppData\\Local\\Temp\\ipykernel_19312\\2236833996.py:5 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create SparkContext\n",
    "sc = SparkContext(\"local\", \"AccumulatorMultipleExample\")\n",
    "\n",
    "# Sample log data\n",
    "log_data = [\n",
    "    \"INFO Server started\",\n",
    "    \"WARN Disk space low\",\n",
    "    \"ERROR Connection failed\",\n",
    "    \"INFO Request processed\",\n",
    "    \"ERROR Disk read failed\",\n",
    "    \"WARN CPU usage high\"\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "logs_rdd = sc.parallelize(log_data)\n",
    "\n",
    "# Create two accumulators\n",
    "error_count = sc.accumulator(0)\n",
    "warn_count = sc.accumulator(0)\n",
    "\n",
    "# Function to update accumulators\n",
    "def check_levels(line):\n",
    "    global error_count, warn_count\n",
    "    if \"ERROR\" in line:\n",
    "        error_count += 1\n",
    "    if \"WARN\" in line:\n",
    "        warn_count += 1\n",
    "    return line  # just a dummy return, as we're not transforming data here\n",
    "\n",
    "# Trigger the function by using an action (e.g., foreach)\n",
    "logs_rdd.foreach(check_levels)\n",
    "\n",
    "\n",
    "\n",
    "# Access the accumulator values from the driver\n",
    "print(\"Number of ERROR lines:\", error_count.value)\n",
    "print(\"Number of WARN lines:\", warn_count.value)\n",
    "\n",
    "input()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd327f64-0c31-4b3d-b1cc-77bbd8573b64",
   "metadata": {},
   "source": [
    "# SPARQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8e3dac-efd7-407b-a3f3-a815d49c9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "datardd=sc.textFile(\"cr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16942822-c847-40cf-81f2-429a8202f925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cid,cname,revenue', '1,A,45']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datardd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7af5dfe-9b50-4cc1-bb60-ddee6c5fd500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cid,cname,revenue'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datardd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d680d6-a805-498c-8894-51e9dec38745",
   "metadata": {},
   "outputs": [],
   "source": [
    "header=datardd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e1c8f8-9ae4-43ee-9ef4-7e34d74b3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "flrdd=datardd.filter(lambda x: x!=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b9396e9-24fd-495d-af14-fa446daa773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,A,45', '2,B,55', '3,C,65']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb39547c-b4a4-4c2f-921a-09a920ddb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitrdd=flrdd.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd6b3d6e-69c6-410f-93fd-c44ed742ef29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'A', '45'],\n",
       " ['2', 'B', '55'],\n",
       " ['3', 'C', '65'],\n",
       " ['4', 'D', '18'],\n",
       " ['1', 'A', '45']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e3c666-e28a-4c0b-adb0-a296887aaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "kvrdd=splitrdd.map(lambda x: (x[1], int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d0e5d1d-862d-4a1e-8ec6-9053e9c67ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 45), ('B', 55), ('C', 65)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc0d760a-261c-44a3-a2bb-e4fce0811b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupbykdd=kvrdd.groupByKey().map(lambda x: (x[0], sum(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bc864df-98bd-42d8-9741-1169736233ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 90)\n",
      "('D', 18)\n",
      "('B', 110)\n",
      "('C', 130)\n"
     ]
    }
   ],
   "source": [
    "for i in groupbykdd.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d7ba74-57c2-4477-9519-74d421447529",
   "metadata": {},
   "source": [
    "# same with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a963cb5c-b433-4222-8b92-50f7c6cc8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(r\"cr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e310d5f-5da9-488d-a96a-5c1c5a2f5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|cname|sum(revenue)|\n",
      "+-----+------------+\n",
      "|    B|         110|\n",
      "|    D|          18|\n",
      "|    C|         130|\n",
      "|    A|          90|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.groupBy(\"cname\").sum(\"revenue\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8637f-823a-42ca-8321-1e16adb5d67f",
   "metadata": {},
   "source": [
    "# pure like sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dadef65-2328-4c51-a6ed-79b65bcf9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.createOrReplaceTempView(\"cr_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cd86a53-5d88-46be-a48d-a36f81e2affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d2c7c5f-dc90-4e25-97b3-a5bca9688c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|cname|sum(revenue)|\n",
      "+-----+------------+\n",
      "|    B|         110|\n",
      "|    D|          18|\n",
      "|    C|         130|\n",
      "|    A|          90|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select cname, sum(revenue) from cr_view group by cname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8bf3136-2365-41b2-97e4-1f0b065ca790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cid: integer (nullable = true)\n",
      " |-- cname: string (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd71517d-ac64-496a-bd4c-6d3209120642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cid=1, cname='A', revenue=45),\n",
       " Row(cid=2, cname='B', revenue=55),\n",
       " Row(cid=3, cname='C', revenue=65),\n",
       " Row(cid=4, cname='D', revenue=18),\n",
       " Row(cid=1, cname='A', revenue=45),\n",
       " Row(cid=2, cname='B', revenue=55),\n",
       " Row(cid=3, cname='C', revenue=65)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDF.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94be52b-c409-4b1f-a183-b22a8dc09504",
   "metadata": {},
   "source": [
    "# Catalyst Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62e33b9c-a8b1-4ee3-bcc3-707fb7ffac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "crDF = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(r\"cr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96be7cdc-a91f-4575-8d89-a8c310ba5fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cid: integer (nullable = true)\n",
      " |-- cname: string (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab81202a-6eff-4379-a42d-60dd0cd29119",
   "metadata": {},
   "outputs": [],
   "source": [
    "crDF.createOrReplaceTempView(\"cr_view\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f16a113-2c97-4cac-92e2-bd80bba59821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-02-21 16:19:50.947\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `gcu` cannot be resolved. Did you mean one of the following? [`cid`, `cname`, `revenue`]. SQLSTATE: 42703\", \"context\": {\"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o27.sql.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `gcu` cannot be resolved. Did you mean one of the following? [`cid`, `cname`, `revenue`]. SQLSTATE: 42703; line 1 pos 7;\\n'Aggregate ['gcu], ['gcu, unresolvedalias('sum('Lahore))]\\n+- SubqueryAlias cr_view\\n   +- View (`cr_view`, [cid#74, cname#75, revenue#76])\\n      +- Relation [cid#74,cname#75,revenue#76] csv\\n\\r\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\r\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\r\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\r\\n\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\r\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\r\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:532)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:502)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:537)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:92)\\r\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\r\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\r\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\r\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\r\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\r\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\r\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\r\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\r\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\r\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\r\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\r\\n\\tat java.base/java.lang.Thread.run(Thread.java:842)\\r\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\r\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\r\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\r\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\r\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\r\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\r\\n\\t\\t... 23 more\\r\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"C:\\\\Users\\\\Dr.Sahib\\\\anaconda3\\\\envs\\\\spark1\\\\Lib\\\\site-packages\\\\pyspark\\\\errors\\\\exceptions\\\\captured.py\", \"line\": \"263\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"C:\\\\Users\\\\Dr.Sahib\\\\anaconda3\\\\envs\\\\spark1\\\\Lib\\\\site-packages\\\\py4j\\\\protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `gcu` cannot be resolved. Did you mean one of the following? [`cid`, `cname`, `revenue`]. SQLSTATE: 42703; line 1 pos 7;\n'Aggregate ['gcu], ['gcu, unresolvedalias('sum('Lahore))]\n+- SubqueryAlias cr_view\n   +- View (`cr_view`, [cid#74, cname#75, revenue#76])\n      +- Relation [cid#74,cname#75,revenue#76] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mselect gcu, sum(Lahore) from cr_view group by  gcu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\session.py:1816\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1812\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1813\u001b[39m             errorClass=\u001b[33m\"\u001b[39m\u001b[33mINVALID_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1814\u001b[39m             messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1815\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1816\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `gcu` cannot be resolved. Did you mean one of the following? [`cid`, `cname`, `revenue`]. SQLSTATE: 42703; line 1 pos 7;\n'Aggregate ['gcu], ['gcu, unresolvedalias('sum('Lahore))]\n+- SubqueryAlias cr_view\n   +- View (`cr_view`, [cid#74, cname#75, revenue#76])\n      +- Relation [cid#74,cname#75,revenue#76] csv\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select gcu, sum(Lahore) from cr_view group by  gcu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7ebce3b-2cef-47cb-aca7-6e9d691199f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Catalog.listTables of <pyspark.sql.catalog.Catalog object at 0x00000162233424D0>>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e061cec0-f522-42d1-bb99-513f0892315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|cname|sum(revenue)|\n",
      "+-----+------------+\n",
      "|    B|         110|\n",
      "|    D|          18|\n",
      "|    C|         130|\n",
      "|    A|          90|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crDF.groupBy(\"cname\").sum(\"revenue\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95844907-07c3-429a-846c-359f4f4d18d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|cname|sum(revenue)|\n",
      "+-----+------------+\n",
      "|    B|         110|\n",
      "|    D|          18|\n",
      "|    C|         130|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crDF.groupBy(\"cname\").sum(\"revenue\").where(crDF.cname != \"A\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4ac14e4-e51e-4918-80c9-3b8d096d59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[cname#75], functions=[sum(revenue#76)])\n",
      "   +- Exchange hashpartitioning(cname#75, 200), ENSURE_REQUIREMENTS, [plan_id=244]\n",
      "      +- HashAggregate(keys=[cname#75], functions=[partial_sum(revenue#76)])\n",
      "         +- Filter (isnotnull(cname#75) AND NOT (cname#75 = A))\n",
      "            +- FileScan csv [cname#75,revenue#76] Batched: false, DataFilters: [isnotnull(cname#75), NOT (cname#75 = A)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Dr.Sahib/spark/cr.txt], PartitionFilters: [], PushedFilters: [IsNotNull(cname), Not(EqualTo(cname,A))], ReadSchema: struct<cname:string,revenue:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crDF.groupBy(\"cname\").sum(\"revenue\").where(crDF.cname != \"A\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c37d0854-2a0a-49bb-a44d-a7b70a8be348",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\clientserver.py:527\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionResetError\u001b[39m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\clientserver.py:530\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    529\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mError while sending or receiving.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[32m    531\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError while sending\u001b[39m\u001b[33m\"\u001b[39m, e, proto.ERROR_ON_SEND)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcrDF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.sum(\u001b[33m\"\u001b[39m\u001b[33mrevenue\u001b[39m\u001b[33m\"\u001b[39m).where(crDF.cname != \u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m).explain(extended=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:1053\u001b[39m, in \u001b[36mDataFrame.groupBy\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgroupBy\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrNameOrOrdinal\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mGroupedData\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     jgd = \u001b[38;5;28mself\u001b[39m._jdf.groupBy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols_ordinal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupedData\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GroupedData(jgd, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:935\u001b[39m, in \u001b[36mDataFrame._jcols_ordinal\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         _cols.append(c)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m935\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:901\u001b[39m, in \u001b[36mDataFrame._jseq\u001b[39m\u001b[34m(self, cols, converter)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_jseq\u001b[39m(\n\u001b[32m    896\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    897\u001b[39m     cols: Sequence,\n\u001b[32m    898\u001b[39m     converter: Optional[Callable[..., Union[\u001b[33m\"\u001b[39m\u001b[33mPrimitiveType\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mJavaObject\u001b[39m\u001b[33m\"\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    899\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mJavaObject\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    900\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\classic\\column.py:104\u001b[39m, in \u001b[36m_to_seq\u001b[39m\u001b[34m(sc, cols, converter)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[32m     99\u001b[39m \n\u001b[32m    100\u001b[39m \u001b[33;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03minto JVM Column objects.\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     cols = \u001b[43m[\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonUtils.toSeq(cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\classic\\column.py:104\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[32m     99\u001b[39m \n\u001b[32m    100\u001b[39m \u001b[33;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03minto JVM Column objects.\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     cols = [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonUtils.toSeq(cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\classic\\column.py:69\u001b[39m, in \u001b[36m_to_java_column\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m     67\u001b[39m     jcol = col._jc\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     jcol = \u001b[43m_create_column_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m     72\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN_OR_STR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m     74\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\pyspark\\sql\\classic\\column.py:62\u001b[39m, in \u001b[36m_create_column_from_name\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjava_gateway\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JVMView\n\u001b[32m     61\u001b[39m sc = get_active_spark_context()\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVMView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctions\u001b[49m.col(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\java_gateway.py:1752\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == UserHelpAutoCompletion.KEY:\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1755\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n\u001b[32m   1757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m._gateway_client, jvm_id=\u001b[38;5;28mself\u001b[39m._id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\java_gateway.py:1057\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1055\u001b[39m         retry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1056\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1059\u001b[39m     logging.exception(\n\u001b[32m   1060\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\spark1\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "crDF.groupBy(\"cname\").sum(\"revenue\").where(crDF.cname != \"A\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6396f-e781-48a0-ad04-8dacf9eae5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
