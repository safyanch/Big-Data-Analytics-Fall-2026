{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9bc44b-09a6-4541-8b39-b47bd0d4f4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "#Create a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"SparkWriterJob\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
    "            .config(\"spark.default.parallelism\", 2)\\\n",
    "            .master(\"local[2]\")\\\n",
    "            .getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b02e5bf-afbf-464c-946b-3530214e8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"C:\\\\hadoop\\\\etc\\\\hadoop\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3088741-a6f5-4492-bbff-f5ea60ea5266",
   "metadata": {},
   "source": [
    "### 03.01 Reading Files into Spark\n",
    "\n",
    "Data can be read into Apache Spark data frames from a variety of data sources. \n",
    "\n",
    "examples : \n",
    "- A flat file on a local disk\n",
    "- A file from HDFS\n",
    "- A Kafka Topic\n",
    "\n",
    "\n",
    "In this example, we will read a CSV file in a HDFS folder into a Spark Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3a9487-15dc-4c62-8b09-e6e1feaac1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Customer: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Rate: double (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      "\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n",
      "|  2|LinkedIn| Headset|2019/11/25|       5| 36.9|  Urgent:Pickup|\n",
      "|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           NULL|\n",
      "|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n",
      "|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the raw CSV file int a Spark DataFrame\n",
    "#    Use inferSchema to infer the schema automatically from the CSV file\n",
    "\n",
    "raw_sales_data = spark\\\n",
    "                .read\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(\"datasets/sales_orders.csv\")\n",
    "\n",
    "#Print the schema for verification\n",
    "raw_sales_data.printSchema();\n",
    "\n",
    "#Print the first 5 records for verification\n",
    "raw_sales_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c47c7-2ac6-43d3-87ec-4a78c915f69a",
   "metadata": {},
   "source": [
    "### 03.02 Writing to HDFS\n",
    "\n",
    "Write the rawSalesData Data Frame into HDFS as a Parquet file. Use Parquet as the format since it enables splitting and filtering. Use GZIP as the compression codec. \n",
    "\n",
    "On completion, verify if the files are correctly through the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975d23e7-5dba-41bb-9335-58ab11190d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_sales_data.write\\\n",
    "#    .option(\"compression\", \"gzip\")\\\n",
    "#    .parquet(\"file:///C:/dummy_hdfs/raw_parquet\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "\n",
    "raw_sales_data.write\\\n",
    "           .option(\"compression\", \"gzip\")\\\n",
    "           .parquet(path=\"dummy_hdfs/raw_parquet\",\n",
    "                    mode=\"overwrite\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2aff0-68ca-411c-aaff-359ed04c6f2f",
   "metadata": {},
   "source": [
    "### 03.03 Write to HDFS with partitioning\n",
    "\n",
    "Write a partitioned Parquet file in HDFS. Partition will be done by Product. This will create one directory per unique product available in the raw CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d02fc4-12cc-4b1b-b53d-2a24a44594c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sales_data.write\\\n",
    "            .option(\"compression\", \"gzip\")\\\n",
    "            .partitionBy(\"Product\")\\\n",
    "            .parquet(path=\"dummy_hdfs/partitioned_parquet\",\n",
    "                    mode=\"overwrite\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ca0df-4dab-4ad8-b4a9-df540df64749",
   "metadata": {},
   "source": [
    "### 03.04 Writing to Hive with Bucketing\n",
    "\n",
    "Create a Bucketed Hive table for orders. Bucketing will be done by Product. It will create 3 buckets based on the hash generated by Product. Hive tables can be queried through SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "349bea20-92ea-4c04-adfc-edf50625da58",
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkRuntimeException",
     "evalue": "[LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`product_bucket_table`, as its associated location 'file:/C:/Users/Dr.Sahib/spark/spark-warehouse/product_bucket_table' already exists. Please pick a different table name, or remove the existing location first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Make sure that the \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mraw_sales_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucketBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProduct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_bucket_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Spark Hive table is stored in spark-warehouse folder\u001b[39;00m\n\u001b[0;32m      9\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOW tables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mSparkRuntimeException\u001b[0m: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`product_bucket_table`, as its associated location 'file:/C:/Users/Dr.Sahib/spark/spark-warehouse/product_bucket_table' already exists. Please pick a different table name, or remove the existing location first."
     ]
    }
   ],
   "source": [
    "\n",
    "#Make sure that the \n",
    "raw_sales_data.write\\\n",
    "            .format(\"parquet\")\\\n",
    "            .bucketBy(3, \"Product\")\\\n",
    "            .saveAsTable(\"product_bucket_table\")\n",
    "            \n",
    "#Spark Hive table is stored in spark-warehouse folder\n",
    "\n",
    "spark.sql(\"SHOW tables\").show(5)\n",
    "\n",
    "#Read bucketed data\n",
    "spark.sql(f\"\"\"\n",
    "        SELECT * FROM product_bucket_table \n",
    "        WHERE Product='Mouse'\"\"\")\\\n",
    "    .show(5)\n",
    "#While the files are persisted to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83c3463-0f83-415e-8fb3-4a431dedc931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/C:/Users/Dr.Sahib/spark/spark-warehouse')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4d3ca-27a9-433b-975d-10e83a9f4926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
